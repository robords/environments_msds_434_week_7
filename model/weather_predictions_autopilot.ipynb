{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "63a7e54b-4bad-4ee5-8a30-daa2e17b74cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d84daf3-8c3f-4478-af58-c051dee7f3ce",
   "metadata": {},
   "source": [
    "# Weather Predictions on AWS Autopilot with NOAA Data\n",
    "\n",
    "https://docs.opendata.aws/noaa-ghcn-pds/readme.html\n",
    "\n",
    "Get the data from public S3 and copy it to my new bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed7ab519-49ab-4f48-a268-311e08676008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://noaa-ghcn-pds/ghcnd-stations.txt to weather/ghcnd-stations.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# get and cleanup the stations file\n",
    "aws s3 cp s3://noaa-ghcn-pds/ghcnd-stations.txt ./weather/ghcnd-stations.txt \n",
    "python3 stations_cleanup.py\n",
    "\n",
    "# upload it\n",
    "aws s3 cp ./weather/stations.csv s3://raw-weather-data/ghcnd-stations.csv\n",
    "\n",
    "# Clean up the temp files and directory\n",
    "rm ./weather/ghcnd-stations.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653d949-f8ff-4efe-b1a2-1e7d8d647209",
   "metadata": {},
   "source": [
    "Let's try to only generate files for the five core elements:\n",
    "\n",
    "* PRCP = Precipitation (tenths of mm)\n",
    "* SNOW = Snowfall (mm)\n",
    "* SNWD = Snow depth (mm)\n",
    "* TMAX = Maximum temperature (tenths of degrees C)\n",
    "* TMIN = Minimum temperature (tenths of degrees C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff86b3-2e3a-4543-880c-840ae6e3ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for VARIABLE in 2010 2011 2012 2013 2014 2015 2016 2018 2019 2020 2021 2022; do\n",
    "    # Get the file\n",
    "    aws s3 cp s3://noaa-ghcn-pds/csv.gz/\"$VARIABLE\".csv.gz ./weather/\"$VARIABLE\".csv.gz\n",
    "    # Decompress the zip file into a temp directory\n",
    "    gzip -d ./weather/\"$VARIABLE\".csv.gz\n",
    "    # Add headers\n",
    "    { echo 'id,date,element,value,M-FLAG,Q-FLAG,S-FLAG,OBS-TIME'; cat ./weather/\"$VARIABLE\".csv; } > ./weather/\"$VARIABLE\"_with_headers.csv\n",
    "    # filter out the columns with bad data\n",
    "    awk -F '\",\"'  'BEGIN {OFS=\",\"} { if ((toupper($6) == \"\"))  print }' ./weather/\"$VARIABLE\"_with_headers.csv > ./weather/\"$VARIABLE\"_filtered.csv\n",
    "    # create a separate file for each value in the third column\n",
    "    awk -v year=$VARIABLE -F ',' '{print >> (\"./weather/\" year \"/\" $3 \".csv\")}' ./weather/\"$VARIABLE\"_filtered.csv\n",
    "    # Combine the stations data in and add headers back to the remaining files\n",
    "    for ELEMENT in PRCP SNOW SNWD TMAX TMIN; do\n",
    "        {\n",
    "            join -t, <(sort ./weather/\"$VARIABLE\"/\"$ELEMENT\".csv) <(sed 1d ./weather/stations.csv | sort)\n",
    "        } > ./weather/\"$VARIABLE\"/\"$ELEMENT\"_combined.csv\n",
    "        { \n",
    "            echo 'id,date,element,reported_value,M-FLAG,Q-FLAG,S-FLAG,OBS-TIME,location'; cat ./weather/\"$VARIABLE\"/\"$ELEMENT\"_combined.csv; \n",
    "        } > ./weather/\"$VARIABLE\"/\"$ELEMENT\"_with_headers.csv\n",
    "        {\n",
    "            cut -d , -f2,4,9 < ./weather/\"$VARIABLE\"/\"$ELEMENT\"_with_headers.csv; \n",
    "        } > ./weather/\"$VARIABLE\"/\"$ELEMENT\"_cut.csv\n",
    "        # Dates need dashes in them\n",
    "        sed -r 's/^(.{4})(.{2})/\\1-\\2-/;s/$//' ./weather/\"$VARIABLE\"/\"$ELEMENT\"_cut.csv > ./weather/\"$VARIABLE\"/\"$ELEMENT\"_edited.csv\n",
    "        # Sync up the contents of the temp directory to S3 prefix\n",
    "        aws s3 cp ./weather/\"$VARIABLE\"/\"$ELEMENT\"_edited.csv s3://raw-weather-data/\"$ELEMENT\"/\"$VARIABLE\".csv\n",
    "    done\n",
    "    # delete all files except those with _with_headers.csv\n",
    "    ls -d -1 \"$PWD/weather/$VARIABLE/\"*.* | egrep -v \"_edited.csv\" | xargs rm\n",
    "    # Clean up the temp files and directory\n",
    "    rm ./weather/\"$VARIABLE\"_with_headers.csv ./weather/\"$VARIABLE\".csv*\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "122d7f00-0489-484b-a7dd-50cb2d7fb07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date-</th>\n",
       "      <th>r-eported_value</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>AS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-02</td>\n",
       "      <td>0</td>\n",
       "      <td>AS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>AS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>AS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>0</td>\n",
       "      <td>AS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2010-04-06</td>\n",
       "      <td>0</td>\n",
       "      <td>AS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2010-04-07</td>\n",
       "      <td>0</td>\n",
       "      <td>AS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2010-04-08</td>\n",
       "      <td>0</td>\n",
       "      <td>AS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2010-04-09</td>\n",
       "      <td>0</td>\n",
       "      <td>AS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2010-04-10</td>\n",
       "      <td>0</td>\n",
       "      <td>AS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date-  r-eported_value location\n",
       "0   2010-01-01                0       AS\n",
       "1   2010-01-02                0       AS\n",
       "2   2010-01-03                0       AS\n",
       "3   2010-01-04                0       AS\n",
       "4   2010-01-05                0       AS\n",
       "..         ...              ...      ...\n",
       "95  2010-04-06                0       AS\n",
       "96  2010-04-07                0       AS\n",
       "97  2010-04-08                0       AS\n",
       "98  2010-04-09                0       AS\n",
       "99  2010-04-10                0       AS\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./weather/2010/SNOW_edited.csv', nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f8553204-c140-4aa3-b8d9-0176d81e7f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 URIs\n",
      "S3 snow URI: s3://raw-weather-data/SNOW/\n",
      "S3 snow URI: s3://raw-weather-data/PRCP/\n",
      "S3 snow URI: s3://raw-weather-data/SNWD/\n",
      "S3 snow URI: s3://raw-weather-data/TMAX/\n",
      "S3 snow URI: s3://raw-weather-data/TMIN/\n"
     ]
    }
   ],
   "source": [
    "SNOW_S3_Path = 's3://raw-weather-data/SNOW/'\n",
    "PRCP_S3_Path = 's3://raw-weather-data/PRCP/'\n",
    "SNWD_S3_Path = 's3://raw-weather-data/SNWD/'\n",
    "TMAX_S3_Path = 's3://raw-weather-data/TMAX/'\n",
    "TMIN_S3_Path = 's3://raw-weather-data/TMIN/'\n",
    "\n",
    "print('S3 URIs')\n",
    "print(f'S3 snow URI: {SNOW_S3_Path}')\n",
    "print(f'S3 snow URI: {PRCP_S3_Path}')\n",
    "print(f'S3 snow URI: {SNWD_S3_Path}')\n",
    "print(f'S3 snow URI: {TMAX_S3_Path}')\n",
    "print(f'S3 snow URI: {TMIN_S3_Path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea0e268-4ae4-4e2b-afc5-c40a61ea0d7f",
   "metadata": {},
   "source": [
    "# Amazon Forecast\n",
    "\n",
    "For this, we'll be using Amazon Forecast:\n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/forecast.html\n",
    "\n",
    "Demo: https://github.com/aws-samples/amazon-forecast-samples/blob/main/notebooks/basic/Getting_Started/Amazon_Forecast_Quick_Start_Guide.ipynb\n",
    "\n",
    "## Get the IAM Role ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "43569288-1753-47cb-a696-476ea5e9686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = boto3.client('iam').get_role(RoleName='ForecastNotebookRole')\n",
    "role_arn = role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fc588569-4d61-4021-b1ca-1ce22775c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('forecast')\n",
    "forecastquery = boto3.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede608b-e04e-4867-83fe-be4036d1bdc5",
   "metadata": {},
   "source": [
    "## Create the Dataset\n",
    "\n",
    "```\n",
    "response = client.create_dataset(\n",
    "    DatasetName='string',\n",
    "    Domain='RETAIL'|'CUSTOM'|'INVENTORY_PLANNING'|'EC2_CAPACITY'|'WORK_FORCE'|'WEB_TRAFFIC'|'METRICS',\n",
    "    DatasetType='TARGET_TIME_SERIES'|'RELATED_TIME_SERIES'|'ITEM_METADATA',\n",
    "    DataFrequency='string',\n",
    "    Schema={\n",
    "        'Attributes': [\n",
    "            {\n",
    "                'AttributeName': 'string',\n",
    "                'AttributeType': 'string'|'integer'|'float'|'timestamp'|'geolocation'\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    EncryptionConfig={\n",
    "        'RoleArn': 'string',\n",
    "        'KMSKeyArn': 'string'\n",
    "    },\n",
    "    Tags=[\n",
    "        {\n",
    "            'Key': 'string',\n",
    "            'Value': 'string'\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a156d9b-b136-4650-95e2-ee9ace750c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_predictions_schema = {\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"target_value\",\n",
    "         \"AttributeType\":\"integer\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "\n",
    "create_dataset_response = client.create_dataset(\n",
    "    DatasetName='Weather_Predictions_Time_Series_MSDS_434',\n",
    "    Domain='CUSTOM',\n",
    "    DatasetType='RELATED_TIME_SERIES',\n",
    "    DataFrequency='1D',\n",
    "    Schema=weather_predictions_schema\n",
    ")\n",
    "\n",
    "\n",
    "ts_dataset_arn = create_dataset_response['DatasetArn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e9a58cf1-0158-4215-9418-2b06449e2cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dataset with ARN arn:aws:forecast:us-east-1:669437599565:dataset/Weather_Predictions_Time_Series_MSDS_434 is now ACTIVE.\n"
     ]
    }
   ],
   "source": [
    "describe_dataset_response = client.describe_dataset(DatasetArn=ts_dataset_arn)\n",
    "\n",
    "print(f\"The Dataset with ARN {ts_dataset_arn} is now {describe_dataset_response['Status']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac3f7ec-7703-4fa1-81c7-3dd6f72f78fd",
   "metadata": {},
   "source": [
    "## Import the Dataset\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd hh:mm:ss\"\n",
    "TS_IMPORT_JOB_NAME = \"TAXI_TTS_IMPORT\"\n",
    "TIMEZONE = \"EST\"\n",
    "\n",
    "ts_dataset_import_job_response = \\\n",
    "    forecast.create_dataset_import_job(DatasetImportJobName=TS_IMPORT_JOB_NAME,\n",
    "                                       DatasetArn=ts_dataset_arn,\n",
    "                                       DataSource= {\n",
    "                                         \"S3Config\" : {\n",
    "                                             \"Path\": ts_s3_path,\n",
    "                                             \"RoleArn\": role_arn\n",
    "                                         } \n",
    "                                       },\n",
    "                                       TimestampFormat=TIMESTAMP_FORMAT,\n",
    "                                       TimeZone = TIMEZONE)\n",
    "\n",
    "ts_dataset_import_job_arn = ts_dataset_import_job_response['DatasetImportJobArn']\n",
    "describe_dataset_import_job_response = forecast.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn)\n",
    "\n",
    "print(f\"Waiting for Dataset Import Job with ARN {ts_dataset_import_job_arn} to become ACTIVE. This process could take 5-10 minutes.\\n\\nCurrent Status:\")\n",
    "\n",
    "status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn))\n",
    "\n",
    "describe_dataset_import_job_response = forecast.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn)\n",
    "print(f\"\\n\\nThe Dataset Import Job with ARN {ts_dataset_import_job_arn} is now {describe_dataset_import_job_response['Status']}.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d1e9ffef-b634-4d5a-ac51-bd375eec80c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidInputException",
     "evalue": "An error occurred (InvalidInputException) when calling the CreateDatasetImportJob operation: Input data has invalid timestamp value: 20100101, Please ensure timestamp values match the specified format: yyyy-MM-dd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidInputException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-61e9076d9ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                          } \n\u001b[1;32m     13\u001b[0m                                        },\n\u001b[0;32m---> 14\u001b[0;31m                                        TimestampFormat=TIMESTAMP_FORMAT)#,\n\u001b[0m\u001b[1;32m     15\u001b[0m                                        \u001b[0;31m#TimeZone = TIMEZONE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m                 )\n\u001b[1;32m    511\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidInputException\u001b[0m: An error occurred (InvalidInputException) when calling the CreateDatasetImportJob operation: Input data has invalid timestamp value: 20100101, Please ensure timestamp values match the specified format: yyyy-MM-dd"
     ]
    }
   ],
   "source": [
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd\"\n",
    "TS_IMPORT_JOB_NAME = \"Snow_Prediction_Import_Job\"\n",
    "TIMEZONE = \"EST\"\n",
    "\n",
    "ts_dataset_import_job_response = \\\n",
    "    client.create_dataset_import_job(DatasetImportJobName=TS_IMPORT_JOB_NAME,\n",
    "                                       DatasetArn=ts_dataset_arn,\n",
    "                                       DataSource= {\n",
    "                                         \"S3Config\" : {\n",
    "                                             \"Path\": SNOW_S3_Path,\n",
    "                                             \"RoleArn\": role_arn\n",
    "                                         } \n",
    "                                       },\n",
    "                                       TimestampFormat=TIMESTAMP_FORMAT)#,\n",
    "                                       #TimeZone = TIMEZONE)\n",
    "\n",
    "ts_dataset_import_job_arn = ts_dataset_import_job_response['DatasetImportJobArn']\n",
    "describe_dataset_import_job_response = client.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn)\n",
    "\n",
    "print(f\"Waiting for Dataset Import Job with ARN {ts_dataset_import_job_arn} to become ACTIVE. This process could take 5-10 minutes.\\n\\nCurrent Status:\")\n",
    "\n",
    "status = util.wait(lambda: client.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn))\n",
    "\n",
    "describe_dataset_import_job_response = client.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn)\n",
    "print(f\"\\n\\nThe Dataset Import Job with ARN {ts_dataset_import_job_arn} is now {describe_dataset_import_job_response['Status']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd0a1b2-eebd-4936-b433-ac6d3bb73ad8",
   "metadata": {},
   "source": [
    "### Creating a DatasetGroup\n",
    "\n",
    "Example:\n",
    "```\n",
    "DATASET_GROUP_NAME = \"TAXI_DEMO\"\n",
    "DATASET_ARNS = [ts_dataset_arn]\n",
    "\n",
    "create_dataset_group_response = \\\n",
    "    forecast.create_dataset_group(Domain=\"CUSTOM\",\n",
    "                                  DatasetGroupName=DATASET_GROUP_NAME,\n",
    "                                  DatasetArns=DATASET_ARNS)\n",
    "\n",
    "dataset_group_arn = create_dataset_group_response['DatasetGroupArn']\n",
    "describe_dataset_group_response = forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)\n",
    "\n",
    "print(f\"The DatasetGroup with ARN {dataset_group_arn} is now {describe_dataset_group_response['Status']}.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d142b-2de3-491c-bb1e-3bd6536ec4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_GROUP_NAME = \"Snow_Forecast\"\n",
    "DATASET_ARNS = [ts_dataset_arn]\n",
    "\n",
    "create_dataset_group_response = \\\n",
    "    forecast.create_dataset_group(Domain=\"CUSTOM\",\n",
    "                                  DatasetGroupName=DATASET_GROUP_NAME,\n",
    "                                  DatasetArns=DATASET_ARNS)\n",
    "\n",
    "dataset_group_arn = create_dataset_group_response['DatasetGroupArn']\n",
    "describe_dataset_group_response = forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)\n",
    "\n",
    "print(f\"The DatasetGroup with ARN {dataset_group_arn} is now {describe_dataset_group_response['Status']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b351d8c-498e-4d34-adfc-d4ffa7927672",
   "metadata": {},
   "source": [
    "## Traing a Predictor\n",
    "\n",
    "Example:\n",
    "```\n",
    "PREDICTOR_NAME = \"TAXI_PREDICTOR\"\n",
    "FORECAST_HORIZON = 24\n",
    "FORECAST_FREQUENCY = \"H\"\n",
    "HOLIDAY_DATASET = [{\n",
    "        'Name': 'holiday',\n",
    "        'Configuration': {\n",
    "        'CountryCode': ['US']\n",
    "    }\n",
    "}]\n",
    "\n",
    "create_auto_predictor_response = \\\n",
    "    forecast.create_auto_predictor(PredictorName = PREDICTOR_NAME,\n",
    "                                   ForecastHorizon = FORECAST_HORIZON,\n",
    "                                   ForecastFrequency = FORECAST_FREQUENCY,\n",
    "                                   DataConfig = {\n",
    "                                       'DatasetGroupArn': dataset_group_arn, \n",
    "                                       'AdditionalDatasets': HOLIDAY_DATASET\n",
    "                                    },\n",
    "                                   ExplainPredictor = True)\n",
    "\n",
    "predictor_arn = create_auto_predictor_response['PredictorArn']\n",
    "print(f\"Waiting for Predictor with ARN {predictor_arn} to become ACTIVE. Depending on data size and predictor settingï¼Œit can take several hours to be ACTIVE.\\n\\nCurrent Status:\")\n",
    "\n",
    "status = util.wait(lambda: forecast.describe_auto_predictor(PredictorArn=predictor_arn))\n",
    "\n",
    "describe_auto_predictor_response = forecast.describe_auto_predictor(PredictorArn=predictor_arn)\n",
    "print(f\"\\n\\nThe Predictor with ARN {predictor_arn} is now {describe_auto_predictor_response['Status']}.\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e6b5bb-ddcf-47f3-b301-cd0cdb63f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTOR_NAME = \"Snow_Predictor\"\n",
    "FORECAST_HORIZON = 180\n",
    "FORECAST_FREQUENCY = \"1D\"\n",
    "\n",
    "create_auto_predictor_response = \\\n",
    "    forecast.create_auto_predictor(PredictorName = PREDICTOR_NAME,\n",
    "                                   ForecastHorizon = FORECAST_HORIZON,\n",
    "                                   ForecastFrequency = FORECAST_FREQUENCY,\n",
    "                                   DataConfig = {\n",
    "                                       'DatasetGroupArn': dataset_group_arn\n",
    "                                    },\n",
    "                                   ExplainPredictor = True)\n",
    "\n",
    "predictor_arn = create_auto_predictor_response['PredictorArn']\n",
    "print(f\"Waiting for Predictor with ARN {predictor_arn} to become ACTIVE. Depending on data size and predictor settingï¼Œit can take several hours to be ACTIVE.\\n\\nCurrent Status:\")\n",
    "\n",
    "status = util.wait(lambda: forecast.describe_auto_predictor(PredictorArn=predictor_arn))\n",
    "\n",
    "describe_auto_predictor_response = forecast.describe_auto_predictor(PredictorArn=predictor_arn)\n",
    "print(f\"\\n\\nThe Predictor with ARN {predictor_arn} is now {describe_auto_predictor_response['Status']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bed7e5-f1a3-4343-95d0-bf876bf3d93e",
   "metadata": {},
   "source": [
    "### Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981fb4b-169b-4864-994f-2defc740abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy_metrics_response = forecast.get_accuracy_metrics(PredictorArn=predictor_arn)\n",
    "wql = get_accuracy_metrics_response['PredictorEvaluationResults'][0]['TestWindows'][0]['Metrics']['WeightedQuantileLosses']\n",
    "accuracy_scores = get_accuracy_metrics_response['PredictorEvaluationResults'][0]['TestWindows'][0]['Metrics']['ErrorMetrics'][0]\n",
    "\n",
    "print(f\"Weighted Quantile Loss (wQL): {json.dumps(wql, indent=2)}\\n\\n\")\n",
    "\n",
    "print(f\"Root Mean Square Error (RMSE): {accuracy_scores['RMSE']}\\n\\n\")\n",
    "\n",
    "print(f\"Weighted Absolute Percentage Error (WAPE): {accuracy_scores['WAPE']}\\n\\n\")\n",
    "\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {accuracy_scores['MAPE']}\\n\\n\")\n",
    "\n",
    "print(f\"Mean Absolute Scaled Error (MASE): {accuracy_scores['MASE']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed218f-d9f9-49bb-b67d-eaf5e62b15a9",
   "metadata": {},
   "source": [
    "## Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842d1c8-377c-4457-9347-d14c453dd3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORECAST_NAME = \"SNOW_FORECAST\"\n",
    "\n",
    "create_forecast_response = \\\n",
    "    forecast.create_forecast(ForecastName=FORECAST_NAME,\n",
    "                             PredictorArn=predictor_arn)\n",
    "\n",
    "forecast_arn = create_forecast_response['ForecastArn']\n",
    "print(f\"Waiting for Forecast with ARN {forecast_arn} to become ACTIVE. Depending on data size and predictor settingsï¼Œit can take several hours to be ACTIVE.\\n\\nCurrent Status:\")\n",
    "\n",
    "status = util.wait(lambda: forecast.describe_forecast(ForecastArn=forecast_arn))\n",
    "\n",
    "describe_forecast_response = forecast.describe_forecast(ForecastArn=forecast_arn)\n",
    "print(f\"\\n\\nThe Forecast with ARN {forecast_arn} is now {describe_forecast_response['Status']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13726f6-37f2-44bf-a09b-4d39c3c2aa9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
