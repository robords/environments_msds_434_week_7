{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a7e54b-4bad-4ee5-8a30-daa2e17b74cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d84daf3-8c3f-4478-af58-c051dee7f3ce",
   "metadata": {},
   "source": [
    "# Weather Predictions on AWS with NOAA Data\n",
    "\n",
    "https://docs.opendata.aws/noaa-ghcn-pds/readme.html\n",
    "\n",
    "Check to see if the files are in S3 first (see below code)\n",
    "\n",
    "If not:\n",
    "Get the data from public S3 and copy it to my new bucket\n",
    "1. Run `bash get_stations.bash`\n",
    "2. Run `bash get_weather_data.bash`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653d949-f8ff-4efe-b1a2-1e7d8d647209",
   "metadata": {},
   "source": [
    "These only generate files for the five core elements:\n",
    "\n",
    "* PRCP = Precipitation (tenths of mm)\n",
    "* SNOW = Snowfall (mm)\n",
    "* SNWD = Snow depth (mm)\n",
    "* TMAX = Maximum temperature (tenths of degrees C)\n",
    "* TMIN = Minimum temperature (tenths of degrees C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8553204-c140-4aa3-b8d9-0176d81e7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the URIs\n",
    "bucket = 'raw-weather-data'\n",
    "path_names = ['SNOW', 'PRCP', 'SNWD', 'TMAX', 'TMIN']\n",
    "paths = []\n",
    "for item in paths:\n",
    "    paths.append(f's3://{bucket}/{item}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "749181c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_s3(bucket, path):\n",
    "    '''\n",
    "    Get the list of files from S3\n",
    "    \n",
    "    If we wanted to just list items in the bucket, we could do the following.  \n",
    "    However, we can't check the file has content with this\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "    for file in my_bucket.objects.all():\n",
    "        print(file.key)\n",
    "    \n",
    "    '''\n",
    "    bucket = bucket\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    response = s3.list_objects_v2(\n",
    "        Bucket=bucket,\n",
    "        Prefix=path\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3c3cd94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNOW\n",
      "[('SNOW/', 0), ('SNOW/2010.csv', 83146517), ('SNOW/2011.csv', 60171285), ('SNOW/2012.csv', 64987919), ('SNOW/2013.csv', 69215906), ('SNOW/2014.csv', 68552703), ('SNOW/2015.csv', 67754976), ('SNOW/2016.csv', 69610408), ('SNOW/2018.csv', 67856373), ('SNOW/2019.csv', 67592700), ('SNOW/2020.csv', 70260824), ('SNOW/2021.csv', 83057402), ('SNOW/2022.csv', 54991998)]\n",
      "PRCP\n",
      "[('PRCP/', 0), ('PRCP/2010.csv', 111588229), ('PRCP/2011.csv', 111580532), ('PRCP/2012.csv', 116675791), ('PRCP/2013.csv', 119863280), ('PRCP/2014.csv', 119868139), ('PRCP/2015.csv', 121048582), ('PRCP/2016.csv', 120887972), ('PRCP/2018.csv', 123466948), ('PRCP/2019.csv', 125105128), ('PRCP/2020.csv', 131016264), ('PRCP/2021.csv', 131048385), ('PRCP/2022.csv', 86813486)]\n",
      "SNWD\n",
      "[('SNWD/', 0), ('SNWD/2010.csv', 56900773), ('SNWD/2011.csv', 33523394), ('SNWD/2012.csv', 34633900), ('SNWD/2013.csv', 37997768), ('SNWD/2014.csv', 38942839), ('SNWD/2015.csv', 39456045), ('SNWD/2016.csv', 42010806), ('SNWD/2018.csv', 42557831), ('SNWD/2019.csv', 42657390), ('SNWD/2020.csv', 42944219), ('SNWD/2021.csv', 41511010), ('SNWD/2022.csv', 27523033)]\n",
      "TMAX\n",
      "[('TMAX/', 0), ('TMAX/2010.csv', 62411233), ('TMAX/2011.csv', 63778562), ('TMAX/2012.csv', 62938894), ('TMAX/2013.csv', 60652150), ('TMAX/2014.csv', 59565423), ('TMAX/2015.csv', 58406992), ('TMAX/2016.csv', 57436181), ('TMAX/2018.csv', 55262707), ('TMAX/2019.csv', 54743233), ('TMAX/2020.csv', 53777508), ('TMAX/2021.csv', 43638630), ('TMAX/2022.csv', 28997431)]\n",
      "TMIN\n",
      "[('TMIN/', 0), ('TMIN/2010.csv', 62024598), ('TMIN/2011.csv', 63453026), ('TMIN/2012.csv', 62417274), ('TMIN/2013.csv', 60468796), ('TMIN/2014.csv', 59197217), ('TMIN/2015.csv', 57981670), ('TMIN/2016.csv', 57036587), ('TMIN/2018.csv', 55136757), ('TMIN/2019.csv', 54526235), ('TMIN/2020.csv', 53521058), ('TMIN/2021.csv', 43401662), ('TMIN/2022.csv', 28925181)]\n"
     ]
    }
   ],
   "source": [
    "for item in path_names:\n",
    "    print(item)\n",
    "    print([(i['Key'], i['Size']) for i in list_files_in_s3(bucket, item)['Contents']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea0e268-4ae4-4e2b-afc5-c40a61ea0d7f",
   "metadata": {},
   "source": [
    "# Amazon Forecast\n",
    "\n",
    "For this, we'll be using Amazon Forecast:\n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/forecast.html\n",
    "\n",
    "Demo: https://github.com/aws-samples/amazon-forecast-samples/blob/main/notebooks/basic/Getting_Started/Amazon_Forecast_Quick_Start_Guide.ipynb\n",
    "\n",
    "## Get the IAM Role ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43569288-1753-47cb-a696-476ea5e9686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = boto3.client('iam').get_role(RoleName='ForecastNotebookRole')\n",
    "role_arn = role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc588569-4d61-4021-b1ca-1ce22775c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('forecast')\n",
    "forecastquery = boto3.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede608b-e04e-4867-83fe-be4036d1bdc5",
   "metadata": {},
   "source": [
    "## Create the Dataset\n",
    "\n",
    "```\n",
    "response = client.create_dataset(\n",
    "    DatasetName='string',\n",
    "    Domain='RETAIL'|'CUSTOM'|'INVENTORY_PLANNING'|'EC2_CAPACITY'|'WORK_FORCE'|'WEB_TRAFFIC'|'METRICS',\n",
    "    DatasetType='TARGET_TIME_SERIES'|'RELATED_TIME_SERIES'|'ITEM_METADATA',\n",
    "    DataFrequency='string',\n",
    "    Schema={\n",
    "        'Attributes': [\n",
    "            {\n",
    "                'AttributeName': 'string',\n",
    "                'AttributeType': 'string'|'integer'|'float'|'timestamp'|'geolocation'\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    EncryptionConfig={\n",
    "        'RoleArn': 'string',\n",
    "        'KMSKeyArn': 'string'\n",
    "    },\n",
    "    Tags=[\n",
    "        {\n",
    "            'Key': 'string',\n",
    "            'Value': 'string'\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a156d9b-b136-4650-95e2-ee9ace750c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Already Exists\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'Weather_Predictions_Time_Series_MSDS_434'\n",
    "dataset_dicts = client.list_datasets()['Datasets']\n",
    "\n",
    "if dataset_name in [i['DatasetName'] for i in client.list_datasets()['Datasets']]:\n",
    "    print('Dataset Already Exists')\n",
    "    ts_dataset_arn = [item for item in dataset_dicts if item[\"DatasetName\"] == dataset_name][0]['DatasetArn']\n",
    "else:\n",
    "    weather_predictions_schema = {\n",
    "       \"Attributes\":[\n",
    "          {\n",
    "             \"AttributeName\":\"timestamp\",\n",
    "             \"AttributeType\":\"timestamp\"\n",
    "          },\n",
    "          {\n",
    "             \"AttributeName\":\"target_value\",\n",
    "             \"AttributeType\":\"integer\"\n",
    "          },\n",
    "          {\n",
    "             \"AttributeName\":\"item_id\",\n",
    "             \"AttributeType\":\"string\"\n",
    "          }\n",
    "       ]\n",
    "    }\n",
    "\n",
    "    # check if the dataset is created first:\n",
    "\n",
    "    create_dataset_response = client.create_dataset(\n",
    "        DatasetName=dataset_name,\n",
    "        Domain='CUSTOM',\n",
    "        DatasetType='TARGET_TIME_SERIES',\n",
    "        DataFrequency='1D',\n",
    "        Schema=weather_predictions_schema\n",
    "    )\n",
    "    ts_dataset_arn = create_dataset_response['DatasetArn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9a58cf1-0158-4215-9418-2b06449e2cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dataset is now ACTIVE.\n"
     ]
    }
   ],
   "source": [
    "describe_dataset_response = client.describe_dataset(DatasetArn=ts_dataset_arn)\n",
    "\n",
    "print(f\"The Dataset is now {describe_dataset_response['Status']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac3f7ec-7703-4fa1-81c7-3dd6f72f78fd",
   "metadata": {},
   "source": [
    "## Import the Dataset\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd hh:mm:ss\"\n",
    "TS_IMPORT_JOB_NAME = \"TAXI_TTS_IMPORT\"\n",
    "TIMEZONE = \"EST\"\n",
    "\n",
    "ts_dataset_import_job_response = \\\n",
    "    forecast.create_dataset_import_job(DatasetImportJobName=TS_IMPORT_JOB_NAME,\n",
    "                                       DatasetArn=ts_dataset_arn,\n",
    "                                       DataSource= {\n",
    "                                         \"S3Config\" : {\n",
    "                                             \"Path\": ts_s3_path,\n",
    "                                             \"RoleArn\": role_arn\n",
    "                                         } \n",
    "                                       },\n",
    "                                       TimestampFormat=TIMESTAMP_FORMAT,\n",
    "                                       TimeZone = TIMEZONE)\n",
    "\n",
    "ts_dataset_import_job_arn = ts_dataset_import_job_response['DatasetImportJobArn']\n",
    "describe_dataset_import_job_response = forecast.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn)\n",
    "\n",
    "print(f\"Waiting for Dataset Import Job with ARN {ts_dataset_import_job_arn} to become ACTIVE. This process could take 5-10 minutes.\\n\\nCurrent Status:\")\n",
    "\n",
    "status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn))\n",
    "\n",
    "describe_dataset_import_job_response = forecast.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn)\n",
    "print(f\"\\n\\nThe Dataset Import Job with ARN {ts_dataset_import_job_arn} is now {describe_dataset_import_job_response['Status']}.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d1e9ffef-b634-4d5a-ac51-bd375eec80c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already Exists\n"
     ]
    }
   ],
   "source": [
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd\"\n",
    "TIMEZONE = \"UTC\"\n",
    "TS_IMPORT_JOB_NAME = \"Snow_Prediction_Import_Job\"\n",
    "import_job = client.list_dataset_import_jobs(\n",
    "    Filters=[\n",
    "        {\n",
    "            'Key': 'DatasetArn',\n",
    "            'Value': ts_dataset_arn,\n",
    "            'Condition': 'IS'\n",
    "        },\n",
    "    ]\n",
    ")   \n",
    "\n",
    "if TS_IMPORT_JOB_NAME in [i['DatasetImportJobName'] for i in import_job['DatasetImportJobs']]:\n",
    "    print('Already Exists')\n",
    "    ts_dataset_import_job_arn = [item for item in import_job['DatasetImportJobs'] if item[\"DatasetImportJobName\"] == TS_IMPORT_JOB_NAME][0]['DatasetImportJobArn']\n",
    "else:\n",
    "    ts_dataset_import_job_response = \\\n",
    "        client.create_dataset_import_job(DatasetImportJobName=TS_IMPORT_JOB_NAME,\n",
    "                                           DatasetArn=ts_dataset_arn,\n",
    "                                           DataSource= {\n",
    "                                             \"S3Config\" : {\n",
    "                                                 \"Path\": SNOW_S3_Path,\n",
    "                                                 \"RoleArn\": role_arn\n",
    "                                             } \n",
    "                                           },\n",
    "                                           TimestampFormat=TIMESTAMP_FORMAT,\n",
    "                                           TimeZone = TIMEZONE)\n",
    "\n",
    "    ts_dataset_import_job_arn = ts_dataset_import_job_response['DatasetImportJobArn']\n",
    "    describe_dataset_import_job_response = client.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn)\n",
    "\n",
    "    print(f\"Waiting for Dataset Import Job with ARNto become ACTIVE. This process could take 5-10 minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "007ef92e-66ce-4808-8ae2-445d62352aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snow_Prediction_Import_Job\n",
      "Status: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = client.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn)\n",
    "print(status['DatasetImportJobName'])\n",
    "try:\n",
    "    print(f'Time Remaining in Minutes (estimated): {status[\"EstimatedTimeRemainingInMinutes\"]}')\n",
    "except:\n",
    "    pass\n",
    "print(f'Status: {status[\"Status\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd0a1b2-eebd-4936-b433-ac6d3bb73ad8",
   "metadata": {},
   "source": [
    "### Creating a DatasetGroup\n",
    "\n",
    "Example:\n",
    "```\n",
    "DATASET_GROUP_NAME = \"TAXI_DEMO\"\n",
    "DATASET_ARNS = [ts_dataset_arn]\n",
    "\n",
    "create_dataset_group_response = \\\n",
    "    forecast.create_dataset_group(Domain=\"CUSTOM\",\n",
    "                                  DatasetGroupName=DATASET_GROUP_NAME,\n",
    "                                  DatasetArns=DATASET_ARNS)\n",
    "\n",
    "dataset_group_arn = create_dataset_group_response['DatasetGroupArn']\n",
    "describe_dataset_group_response = forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)\n",
    "\n",
    "print(f\"The DatasetGroup with ARN {dataset_group_arn} is now {describe_dataset_group_response['Status']}.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "010d142b-2de3-491c-bb1e-3bd6536ec4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already Exists\n"
     ]
    }
   ],
   "source": [
    "DATASET_ARNS = [ts_dataset_arn]\n",
    "\n",
    "DATASET_GROUP_NAME = \"Snow_Forecast\"\n",
    "if DATASET_GROUP_NAME in [i['DatasetGroupName'] for i in client.list_dataset_groups()['DatasetGroups']]:\n",
    "    print('Already Exists')\n",
    "    dataset_group_arn = ([item for item in client.list_dataset_groups()['DatasetGroups'] \n",
    "                         if item[\"DatasetGroupName\"] == DATASET_GROUP_NAME][0]['DatasetGroupArn'])\n",
    "else:\n",
    "    create_dataset_group_response = \\\n",
    "        client.create_dataset_group(Domain=\"CUSTOM\",\n",
    "                                      DatasetGroupName=DATASET_GROUP_NAME,\n",
    "                                      DatasetArns=DATASET_ARNS)\n",
    "\n",
    "    dataset_group_arn = create_dataset_group_response['DatasetGroupArn']\n",
    "    describe_dataset_group_response = client.describe_dataset_group(DatasetGroupArn=dataset_group_arn)\n",
    "\n",
    "    print(f\"The DatasetGroup with ARN {dataset_group_arn} is now {describe_dataset_group_response['Status']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b351d8c-498e-4d34-adfc-d4ffa7927672",
   "metadata": {},
   "source": [
    "## Traing a Predictor\n",
    "\n",
    "Example:\n",
    "```\n",
    "PREDICTOR_NAME = \"TAXI_PREDICTOR\"\n",
    "FORECAST_HORIZON = 24\n",
    "FORECAST_FREQUENCY = \"H\"\n",
    "HOLIDAY_DATASET = [{\n",
    "        'Name': 'holiday',\n",
    "        'Configuration': {\n",
    "        'CountryCode': ['US']\n",
    "    }\n",
    "}]\n",
    "\n",
    "create_auto_predictor_response = \\\n",
    "    forecast.create_auto_predictor(PredictorName = PREDICTOR_NAME,\n",
    "                                   ForecastHorizon = FORECAST_HORIZON,\n",
    "                                   ForecastFrequency = FORECAST_FREQUENCY,\n",
    "                                   DataConfig = {\n",
    "                                       'DatasetGroupArn': dataset_group_arn, \n",
    "                                       'AdditionalDatasets': HOLIDAY_DATASET\n",
    "                                    },\n",
    "                                   ExplainPredictor = True)\n",
    "\n",
    "predictor_arn = create_auto_predictor_response['PredictorArn']\n",
    "print(f\"Waiting for Predictor with ARN {predictor_arn} to become ACTIVE. Depending on data size and predictor setting，it can take several hours to be ACTIVE.\\n\\nCurrent Status:\")\n",
    "\n",
    "status = util.wait(lambda: forecast.describe_auto_predictor(PredictorArn=predictor_arn))\n",
    "\n",
    "describe_auto_predictor_response = forecast.describe_auto_predictor(PredictorArn=predictor_arn)\n",
    "print(f\"\\n\\nThe Predictor with ARN {predictor_arn} is now {describe_auto_predictor_response['Status']}.\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "90e6b5bb-ddcf-47f3-b301-cd0cdb63f5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Predictor with ARN to become ACTIVE. Depending on data size and predictor setting，it can take several hours to be ACTIVE\n"
     ]
    }
   ],
   "source": [
    "FORECAST_HORIZON = 180\n",
    "FORECAST_FREQUENCY = \"1D\"\n",
    "\n",
    "PREDICTOR_NAME = \"Snow_Predictor\"\n",
    "if PREDICTOR_NAME in [i['PredictorName'] for i in client.list_predictors()['Predictors']]:\n",
    "    print('Already Exists')\n",
    "    predictor_arn = ([item for item in client.list_predictors()['Predictors'] \n",
    "                         if item[\"PredictorName\"] == PREDICTOR_NAME][0]['PredictorArn'])\n",
    "else:\n",
    "    create_auto_predictor_response = \\\n",
    "        client.create_auto_predictor(PredictorName = PREDICTOR_NAME,\n",
    "                                       ForecastHorizon = FORECAST_HORIZON,\n",
    "                                       ForecastFrequency = FORECAST_FREQUENCY,\n",
    "                                       DataConfig = {\n",
    "                                           'DatasetGroupArn': dataset_group_arn\n",
    "                                        })\n",
    "\n",
    "    predictor_arn = create_auto_predictor_response['PredictorArn']\n",
    "    print(f\"Waiting for Predictor with ARN to become ACTIVE. Depending on data size and predictor setting，it can take several hours to be ACTIVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "93193684-aab2-47c2-a711-05f32212c45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snow_Predictor\n",
      "Status: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "pred_status = client.describe_auto_predictor(PredictorArn=predictor_arn)\n",
    "\n",
    "print(pred_status['PredictorName'])\n",
    "try:\n",
    "    print(f'Time Remaining in Minutes (estimated): {pred_status[\"EstimatedTimeRemainingInMinutes\"]}')\n",
    "except:\n",
    "    pass\n",
    "print(f'Status: {pred_status[\"Status\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bed7e5-f1a3-4343-95d0-bf876bf3d93e",
   "metadata": {},
   "source": [
    "### Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4981fb4b-169b-4864-994f-2defc740abf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Quantile Loss (wQL): [\n",
      "  {\n",
      "    \"Quantile\": 0.9,\n",
      "    \"LossValue\": 1.6621176413113308\n",
      "  },\n",
      "  {\n",
      "    \"Quantile\": 0.5,\n",
      "    \"LossValue\": 1.0098386699743338\n",
      "  },\n",
      "  {\n",
      "    \"Quantile\": 0.1,\n",
      "    \"LossValue\": 0.23374340045447026\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "Root Mean Square Error (RMSE): 1279.1895539850382\n",
      "\n",
      "\n",
      "Weighted Absolute Percentage Error (WAPE): 1.8649589831575322\n",
      "\n",
      "\n",
      "Mean Absolute Percentage Error (MAPE): 0.8804727600326857\n",
      "\n",
      "\n",
      "Mean Absolute Scaled Error (MASE): 1.939399311943807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_accuracy_metrics_response = client.get_accuracy_metrics(PredictorArn=predictor_arn)\n",
    "wql = get_accuracy_metrics_response['PredictorEvaluationResults'][0]['TestWindows'][0]['Metrics']['WeightedQuantileLosses']\n",
    "accuracy_scores = get_accuracy_metrics_response['PredictorEvaluationResults'][0]['TestWindows'][0]['Metrics']['ErrorMetrics'][0]\n",
    "\n",
    "print(f\"Weighted Quantile Loss (wQL): {json.dumps(wql, indent=2)}\\n\\n\")\n",
    "\n",
    "print(f\"Root Mean Square Error (RMSE): {accuracy_scores['RMSE']}\\n\\n\")\n",
    "\n",
    "print(f\"Weighted Absolute Percentage Error (WAPE): {accuracy_scores['WAPE']}\\n\\n\")\n",
    "\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {accuracy_scores['MAPE']}\\n\\n\")\n",
    "\n",
    "print(f\"Mean Absolute Scaled Error (MASE): {accuracy_scores['MASE']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed218f-d9f9-49bb-b67d-eaf5e62b15a9",
   "metadata": {},
   "source": [
    "## Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842d1c8-377c-4457-9347-d14c453dd3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORECAST_NAME = \"SNOW_FORECAST\"\n",
    "\n",
    "if FORECAST_NAME in [i['ForecastName'] for i in client.list_forecasts()['Forecasts']]:\n",
    "    print('Already Exists')\n",
    "    forecast_arn = ([item for item in client.list_forecasts()['Forecasts'] \n",
    "                         if item[\"ForecastName\"] == FORECAST_NAME][0]['ForecastArn'])\n",
    "else:\n",
    "\n",
    "    create_forecast_response = \\\n",
    "        forecast.create_forecast(ForecastName=FORECAST_NAME,\n",
    "                                 PredictorArn=predictor_arn)\n",
    "\n",
    "    forecast_arn = create_forecast_response['ForecastArn']\n",
    "    print(f\"Waiting for Forecast to become ACTIVE. Depending on data size and predictor settings，it can take several hours to be ACTIVE.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13726f6-37f2-44bf-a09b-4d39c3c2aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_status = client.describe_forecast(ForecastArn=forecast_arn)\n",
    "\n",
    "print(forecast_status['PredictorName'])\n",
    "try:\n",
    "    print(f'Time Remaining in Minutes (estimated): {forecast_status[\"EstimatedTimeRemainingInMinutes\"]}')\n",
    "except:\n",
    "    pass\n",
    "print(f'Status: {forecast_status[\"Status\"]}')"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
