{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e08d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8adc4",
   "metadata": {},
   "source": [
    "# Cost Predictions on AWS\n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ce.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87f9d25",
   "metadata": {},
   "source": [
    "## Get the Cost Data from AWS and save it a CSV in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d62fe331",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_cost_data_from_aws(start_date, end_date):\n",
    "    client = boto3.client('ce')\n",
    "    results = client.get_cost_and_usage(\n",
    "        TimePeriod={\n",
    "            'Start': start_date,\n",
    "            'End': datetime.now().strftime('%Y-%m-%d')\n",
    "            },\n",
    "            Granularity='DAILY',\n",
    "            Metrics=[\n",
    "                'UnblendedCost',\n",
    "            ],\n",
    "            GroupBy=[\n",
    "                {\n",
    "                    'Type': 'DIMENSION',\n",
    "                    'Key': 'SERVICE'\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    return results\n",
    "\n",
    "def get_cost_data_to_pandas(start_date, end_date):\n",
    "    results = get_cost_data_from_aws(start_date, end_date)\n",
    "    \n",
    "    start_date_list = []\n",
    "    cost_list = []\n",
    "    service_list = []\n",
    "\n",
    "    for item in results['ResultsByTime']:\n",
    "        for i in item['Groups']:\n",
    "            start_date_list.append(item['TimePeriod']['Start'])\n",
    "        service_list = service_list + [i['Keys'][0] for i in item['Groups']]\n",
    "        cost_list = cost_list + [i['Metrics']['UnblendedCost']['Amount'] for i in item['Groups']]\n",
    "    df = pd.DataFrame(list(zip(start_date_list, cost_list, service_list)), \n",
    "                      columns =['start_date', 'costs', 'service'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47ff711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2022-09-01'  ## This is the month that class started\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "my_cost_data = get_cost_data_to_pandas(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e89d069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_s3(df, end_date):\n",
    "    '''\n",
    "    Upload the CSV to S3\n",
    "    '''\n",
    "    # save the df as a csv\n",
    "    csv = f'cost_data_to_{end_date}.csv'\n",
    "    df.to_csv(csv, index=False)\n",
    "    \n",
    "    # set up the session\n",
    "    client = boto3.client('s3')\n",
    "\n",
    "    # upload the file\n",
    "    bucket = 'cost-management-robords'\n",
    "    print(f'getting file from {csv}')\n",
    "    key = f'by_service/{csv}'\n",
    "    with open(csv, \"rb\") as f:\n",
    "        client.upload_fileobj(f, bucket, key)\n",
    "\n",
    "    # Remove the file from the local filesystem\n",
    "    command_to_run_rm = [\"rm\", csv]\n",
    "    output_rm = subprocess.check_output(command_to_run_rm).decode(\"utf-8\").strip()\n",
    "    print(output_rm)\n",
    "\n",
    "    print('File uploaded to S3')\n",
    "    \n",
    "    return bucket, key, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7bb28883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting file from cost_data_to_2022-11-07.csv\n",
      "\n",
      "File uploaded to S3\n"
     ]
    }
   ],
   "source": [
    "bucket, key, csv = save_to_s3(my_cost_data, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4948184",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "af132099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cost_forecast:\n",
    "    \n",
    "    def __init__(self, s3_path, dataset_name = 'Cost_Dataset',\n",
    "                import_job_name = \"Cost_Prediction_Import_Job\", dataset_group_name = \"Cost_Forecast\",\n",
    "                forecast_horizon = 30, forecast_frequency = \"1D\", predictor_name = \"Cost_Predictor\",\n",
    "                forecast_name = \"Cost_Forecast\"):\n",
    "        \n",
    "        self.s3_path = s3_path\n",
    "        self.role = boto3.client('iam').get_role(RoleName='ForecastNotebookRole')\n",
    "        self.role_arn = self.role['Role']['Arn']\n",
    "        \n",
    "        self.client = boto3.client('forecast')\n",
    "        \n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset_arn = self.create_dataset()\n",
    "        \n",
    "        while self.check_status('dataset') != 'ACTIVE':\n",
    "            clear_output(wait=True)\n",
    "            self.check_status('dataset')\n",
    "            \n",
    "        self.import_job_name = import_job_name\n",
    "        self.dataset_import_job_arn = self.import_dataset()\n",
    "        \n",
    "        self.dataset_group_name = dataset_group_name\n",
    "        self.dataset_group_arn = self.dataset_group()\n",
    "        \n",
    "        while self.check_status('import') != 'ACTIVE':\n",
    "            clear_output(wait=True)\n",
    "            self.check_status('import')\n",
    "        \n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.forecast_frequency = forecast_frequency\n",
    "        self.predictor_name = predictor_name\n",
    "        self.predictor_arn = self.train_predictor()\n",
    "        \n",
    "        while self.check_status('predictor') != 'ACTIVE':\n",
    "            clear_output(wait=True)\n",
    "            self.check_status('predictor')\n",
    "        \n",
    "        self.forecast = forecast_name\n",
    "        self.forecast_arn = self.create_forecast()\n",
    "        \n",
    "    \n",
    "    def create_dataset(self):\n",
    "        dataset_dicts = self.client.list_datasets()['Datasets']\n",
    "\n",
    "        if self.dataset_name in [i['DatasetName'] for i in self.client.list_datasets()['Datasets']]:\n",
    "            print('Dataset Already Exists')\n",
    "            ts_dataset_arn = ([item for item in dataset_dicts if \n",
    "                               item[\"DatasetName\"] == self.dataset_name][0]['DatasetArn'])\n",
    "        else:\n",
    "            print('Creating New Dataset')\n",
    "            schema = {\n",
    "               \"Attributes\":[\n",
    "                  {\n",
    "                     \"AttributeName\":\"timestamp\",\n",
    "                     \"AttributeType\":\"timestamp\"\n",
    "                  },\n",
    "                  {\n",
    "                     \"AttributeName\":\"target_value\",\n",
    "                     \"AttributeType\":\"float\"\n",
    "                  },\n",
    "                  {\n",
    "                     \"AttributeName\":\"item_id\",\n",
    "                     \"AttributeType\":\"string\"\n",
    "                  }\n",
    "               ]\n",
    "            }\n",
    "\n",
    "            # check if the dataset is created first:\n",
    "\n",
    "            create_dataset_response = self.client.create_dataset(\n",
    "                DatasetName=self.dataset_name,\n",
    "                Domain='CUSTOM',\n",
    "                DatasetType='TARGET_TIME_SERIES',\n",
    "                DataFrequency='1D',\n",
    "                Schema=schema\n",
    "            )\n",
    "            ts_dataset_arn = create_dataset_response['DatasetArn']\n",
    "            print('Dataset Create Initiated')\n",
    "        \n",
    "        return ts_dataset_arn\n",
    "    \n",
    "    def import_dataset(self):\n",
    "        TIMESTAMP_FORMAT = \"yyyy-MM-dd\"\n",
    "        TIMEZONE = \"UTC\"\n",
    "\n",
    "        import_job = self.client.list_dataset_import_jobs(\n",
    "            Filters=[\n",
    "                {\n",
    "                    'Key': 'DatasetArn',\n",
    "                    'Value': self.dataset_arn,\n",
    "                    'Condition': 'IS'\n",
    "                },\n",
    "            ]\n",
    "        )   \n",
    "\n",
    "        if self.import_job_name in [i['DatasetImportJobName'] for i in import_job['DatasetImportJobs']]:\n",
    "            print('Already Exists')\n",
    "            ts_dataset_import_job_arn = [item for item in import_job['DatasetImportJobs'] if item[\"DatasetImportJobName\"] == self.import_job_name][0]['DatasetImportJobArn']\n",
    "        else:\n",
    "            ts_dataset_import_job_response = \\\n",
    "                self.client.create_dataset_import_job(DatasetImportJobName=self.import_job_name,\n",
    "                                                   DatasetArn=self.dataset_arn,\n",
    "                                                   DataSource= {\n",
    "                                                     \"S3Config\" : {\n",
    "                                                         \"Path\": self.s3_path,\n",
    "                                                         \"RoleArn\": self.role_arn\n",
    "                                                     } \n",
    "                                                   },\n",
    "                                                   TimestampFormat=TIMESTAMP_FORMAT,\n",
    "                                                   TimeZone = TIMEZONE)\n",
    "\n",
    "            ts_dataset_import_job_arn = ts_dataset_import_job_response['DatasetImportJobArn']\n",
    "            describe_dataset_import_job_response = self.client.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn)\n",
    "\n",
    "            print(f\"Waiting for Dataset Import Job with ARNto become ACTIVE. This process could take 5-10 minutes.\")\n",
    "        return ts_dataset_import_job_arn\n",
    "    \n",
    "    def dataset_group(self):\n",
    "        DATASET_ARNS = [self.dataset_arn]\n",
    "        \n",
    "        if self.dataset_group_name in [i['DatasetGroupName'] for i in self.client.list_dataset_groups()['DatasetGroups']]:\n",
    "            print('Already Exists')\n",
    "            dataset_group_arn = ([item for item in self.client.list_dataset_groups()['DatasetGroups'] \n",
    "                                 if item[\"DatasetGroupName\"] == self.dataset_group_name][0]['DatasetGroupArn'])\n",
    "        else:\n",
    "            create_dataset_group_response = \\\n",
    "                self.client.create_dataset_group(Domain=\"CUSTOM\",\n",
    "                                              DatasetGroupName=self.dataset_group_name,\n",
    "                                              DatasetArns=DATASET_ARNS)\n",
    "\n",
    "            dataset_group_arn = create_dataset_group_response['DatasetGroupArn']\n",
    "            describe_dataset_group_response = self.client.describe_dataset_group(DatasetGroupArn=dataset_group_arn)\n",
    "\n",
    "            print(f\"The DatasetGroup with ARN {dataset_group_arn} is now {describe_dataset_group_response['Status']}.\")\n",
    "    \n",
    "        return dataset_group_arn\n",
    "    \n",
    "    def train_predictor(self):\n",
    "        \n",
    "        if self.predictor_name in [i['PredictorName'] for i in self.client.list_predictors()['Predictors']]:\n",
    "            print('Already Exists')\n",
    "            predictor_arn = ([item for item in self.client.list_predictors()['Predictors'] \n",
    "                                 if item[\"PredictorName\"] == self.predictor_name][0]['PredictorArn'])\n",
    "        else:\n",
    "            create_auto_predictor_response = \\\n",
    "                self.client.create_auto_predictor(PredictorName = self.predictor_name,\n",
    "                                               ForecastHorizon = self.forecast_horizon,\n",
    "                                               ForecastFrequency = self.forecast_frequency,\n",
    "                                               DataConfig = {\n",
    "                                                   'DatasetGroupArn': self.dataset_group_arn\n",
    "                                                })\n",
    "\n",
    "            predictor_arn = self.create_auto_predictor_response['PredictorArn']\n",
    "            print(f\"Waiting for Predictor with ARN to become ACTIVE. Depending on data size and predictor setting，it can take several hours to be ACTIVE\")\n",
    "        return predictor_arn\n",
    "    \n",
    "    def create_forecast(self):\n",
    "        \n",
    "        if self.forecast_name in [i['ForecastName'] for i in self.client.list_forecasts()['Forecasts']]:\n",
    "            print('Already Exists')\n",
    "            forecast_arn = ([item for item in self.client.list_forecasts()['Forecasts'] \n",
    "                                 if item[\"ForecastName\"] == self.forecast_name][0]['ForecastArn'])\n",
    "        else:\n",
    "\n",
    "            create_forecast_response = \\\n",
    "                self.client.create_forecast(ForecastName=self.forecast_name,\n",
    "                                         PredictorArn=self.predictor_arn)\n",
    "\n",
    "            forecast_arn = create_forecast_response['ForecastArn']\n",
    "            print(f\"Waiting for Forecast to become ACTIVE. Depending on data size and predictor settings，it can take several hours to be ACTIVE.\")\n",
    "\n",
    "        return forecast_arn\n",
    "    \n",
    "    def check_status(self, describe_type):\n",
    "        if describe_type == 'dataset':\n",
    "            describe_dataset_response = self.client.describe_dataset(DatasetArn=self.dataset_arn)\n",
    "            print(f\"The Dataset is now {describe_dataset_response['Status']}.\")\n",
    "            return describe_dataset_response['Status']\n",
    "        \n",
    "        elif describe_type == 'import':\n",
    "            status = self.client.describe_dataset_import_job(DatasetImportJobArn=self.dataset_import_job_arn)\n",
    "            print(status['DatasetImportJobName'])\n",
    "            try:\n",
    "                print(f'Time Remaining in Minutes (estimated): {status[\"EstimatedTimeRemainingInMinutes\"]}')\n",
    "            except:\n",
    "                pass\n",
    "            print(f'Status: {status[\"Status\"]}')\n",
    "            return status[\"Status\"]\n",
    "        \n",
    "        elif describe_type == 'predictor':\n",
    "            pred_status = self.client.describe_auto_predictor(PredictorArn=self.predictor_arn)\n",
    "            print(pred_status['PredictorName'])\n",
    "            try:\n",
    "                print(f'Time Remaining in Minutes (estimated): {pred_status[\"EstimatedTimeRemainingInMinutes\"]}')\n",
    "            except:\n",
    "                pass\n",
    "            print(f'Status: {pred_status[\"Status\"]}')\n",
    "            return pred_status[\"Status\"]\n",
    "        \n",
    "        elif describe_type == 'forecast':\n",
    "            forecast_status = self.client.describe_forecast(ForecastArn=self.forecast_arn)\n",
    "            print(forecast_status['ForecastName'])\n",
    "            try:\n",
    "                print(f'Time Remaining in Minutes (estimated): {forecast_status[\"EstimatedTimeRemainingInMinutes\"]}')\n",
    "            except:\n",
    "                pass\n",
    "            print(f'Status: {forecast_status[\"Status\"]}')\n",
    "            \n",
    "            return forecast_status[\"Status\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6f7ffc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = 's3://'+bucket+'/'+key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a009208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost_Prediction_Import_Job\n",
      "Time Remaining in Minutes (estimated): 30\n",
      "Status: CREATE_IN_PROGRESS\n"
     ]
    }
   ],
   "source": [
    "cost_forecast_output = cost_forecast(s3_path = s3_path, dataset_name = 'Cost_Datasetv2',\n",
    "                import_job_name = \"Cost_Prediction_Import_Job\", dataset_group_name = \"Cost_Forecast\",\n",
    "                forecast_horizon = 30, forecast_frequency = \"1D\", predictor_name = \"Cost_Predictor\",\n",
    "                forecast_name = \"Cost_Forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1d7aa",
   "metadata": {},
   "source": [
    "## Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastquery = boto3.client(service_name='forecastquery')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
